# -*- coding: utf-8 -*-
"""assignment3_sample_(1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bqFoE3TSLOoA7xO6Smc7WD0cddPhnzH_
"""

import transformers as T
from datasets import load_dataset
import torch
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from tqdm import tqdm
from torchmetrics import SpearmanCorrCoef, Accuracy, F1Score
# device = "cuda:3" if torch.cuda.is_available() else "cpu"
device = "cuda:0" if torch.cuda.is_available() else "cpu"

# 使用 GPT 添加註解。

# 有些中文的標點符號在tokenizer編碼以後會變成[UNK]，所以將其換成英文標點
token_replacement = [
    ["：" , ":"],
    ["，" , ","],
    ["“" , "\""],
    ["”" , "\""],
    ["？" , "?"],
    ["……" , "..."],
    ["！" , "!"]
]

# model = MultiLabelModel().to(device) 改倒後面
tokenizer = T.BertTokenizer.from_pretrained("google-bert/bert-base-uncased", cache_dir="./cache/")

class SemevalDataset(Dataset):
    def __init__(self, split="train") -> None:
        super().__init__()
        assert split in ["train", "validation","test"]
        self.data = load_dataset(
            "sem_eval_2014_task_1", split=split, cache_dir="./cache/"
        ).to_list()

    def __getitem__(self, index):
        d = self.data[index]
        # 把中文標點替換掉
        for k in ["premise", "hypothesis"]:
            for tok in token_replacement:
                d[k] = d[k].replace(tok[0], tok[1])
        return d

    def __len__(self):
        return len(self.data)

data_sample = SemevalDataset(split="train").data[:3]
print(f"Dataset example: \n{data_sample[0]} \n{data_sample[1]} \n{data_sample[2]}")

# Define the hyperparameters
lr = 3e-5
epochs = 3
train_batch_size = 8
validation_batch_size = 8

# TODO1: Create batched data for DataLoader
# `collate_fn` is a function that defines how the data batch should be packed.
# This function will be called in the DataLoader to pack the data batch.

def collate_fn(batch):
    # TODO1-1: Implement the collate_fn function
    # Write your code here
    # The input parameter is a data batch (tuple), and this function packs it into tensors.
    # Use tokenizer to pack tokenize and pack the data and its corresponding labels.
    # Return the data batch and labels for each sub-task.
    # 提取 `premise` 和 `hypothesis`
    premises = [item["premise"] for item in batch]
    hypotheses = [item["hypothesis"] for item in batch]
    # 提取 `relatedness_score` 為浮點數 tensor
    relatedness_scores = torch.tensor([item["relatedness_score"] for item in batch], dtype=torch.float)
    # 提取 `entailment_judgment` 為 tensor (已經是數字格式)
    labels = torch.tensor([item["entailment_judgment"] for item in batch], dtype=torch.long)

    # 使用 tokenizer 進行 tokenize
    inputs = tokenizer(
        premises,
        hypotheses,
        padding=True,
        truncation=True,
        max_length=128,
        return_tensors="pt"
    )
    inputs = {key: val.to(device) for key, val in inputs.items()}
    relatedness_scores = relatedness_scores.to(device)
    labels = labels.to(device)

    # 返回 inputs 和標籤
    return inputs, relatedness_scores, labels


# TODO1-2: Define your DataLoader

# 定義訓練和驗證集的 DataLoader
train_dataset = SemevalDataset(split="train")
validation_dataset = SemevalDataset(split="validation")

dl_train = DataLoader(
    train_dataset,
    batch_size=train_batch_size,
    shuffle=True,
    collate_fn=collate_fn
)

dl_validation = DataLoader(
    validation_dataset,
    batch_size=validation_batch_size,
    shuffle=False,
    collate_fn=collate_fn
)

# 測試 DataLoader
sample_inputs, sample_relatedness_scores, sample_labels = next(iter(dl_train))
print("Inputs:", sample_inputs)
print("Relatedness Scores:", sample_relatedness_scores)
print("Labels:", sample_labels)

import torch
from torch import nn
from transformers import BertModel, BertTokenizer

# TODO2: Construct your model
class MultiLabelModel(torch.nn.Module):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # Write your code here
        # Define what modules you will use in the model

        # 載入 BERT 模型
        self.bert = BertModel.from_pretrained("bert-base-uncased")
        # self.bert = BertModel.from_pretrained("bert-large-uncased")
        # self.bert = BertModel.from_pretrained("distilbert-base-uncased")


        # 第一個線性層，用來預測 relatedness_score（回歸任務）
        self.linear_1 = nn.Linear(self.bert.config.hidden_size, 1)

        # 第二個線性層，用來預測 entailment_judgment（分類任務）
        self.linear_2 = nn.Linear(self.bert.config.hidden_size, 3)  # 3 是分類的數量: NEUTRAL, ENTAILMENT, CONTRADICTION

    def forward(self, input_ids, attention_mask, token_type_ids):
      # BERT 的正向傳播
      outputs = self.bert(input_ids=input_ids,
                          attention_mask=attention_mask,
                          token_type_ids=token_type_ids)

      # 提取 BERT 的 [CLS] 標記
      pooled_output = outputs.pooler_output

      # 預測 relatedness_score
      relatedness_score = self.linear_1(pooled_output)

      # 預測 entailment_judgment
      entailment_judgment = self.linear_2(pooled_output)

      # 回傳兩個結果
      return relatedness_score, entailment_judgment

model = MultiLabelModel().to(device)

# TODO3: Define your optimizer and loss function
import torch
from torch import optim
from torchmetrics import SpearmanCorrCoef, Accuracy, F1Score
from torch.nn import MSELoss, CrossEntropyLoss
from torch.optim.lr_scheduler import ReduceLROnPlateau

# TODO3-1: Define your Optimizer
optimizer = optim.AdamW(model.parameters(), lr=lr)


# TODO3-2: Define your loss functions (you should have two)
# Write your code here
loss_fn_relatedness = MSELoss()  # 用於 relatedness_score 的回歸任務
loss_fn_entailment = CrossEntropyLoss()  # 用於 entailment_judgment 的分類任務

# from torch.nn import HuberLoss

# # 定義損失函數
# loss_fn_relatedness = HuberLoss(delta=1.0)  # 回歸損失，使用 Huber Loss
# loss_fn_entailment = CrossEntropyLoss()      # 分類損失，使用 Binary Cross Entropy Loss

# scoring functions
spc = SpearmanCorrCoef()
acc = Accuracy(task="multiclass", num_classes=3)
f1 = F1Score(task="multiclass", num_classes=3, average='macro')

from sklearn.metrics import accuracy_score, f1_score
from scipy.stats import spearmanr
from torchmetrics import SpearmanCorrCoef, Accuracy, F1Score
import torch
from tqdm import tqdm
from torch.optim.lr_scheduler import StepLR
from torch.optim.lr_scheduler import ReduceLROnPlateau

# edit Hyperparameter
epochs = 5
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1)

save_path = "./drive/MyDrive/saved_models"
for ep in range(epochs):
    pbar = tqdm(dl_train)
    pbar.set_description(f"Training epoch [{ep+1}/{epochs}]")
    model.train()

    # current_lr = optimizer.param_groups[0]['lr']

    # print(f"Epoch [{ep+1}/{epochs}] - Learning Rate after scheduler step: {current_lr}")

    # Training loop
    for batch in pbar:
        # Extract data
        input_ids = batch[0]['input_ids'].to(device)
        attention_mask = batch[0]['attention_mask'].to(device)
        token_type_ids = batch[0]['token_type_ids'].to(device)
        relatedness_score = batch[1].to(device)
        entailment_judgment = batch[2].to(device)

        # Zero the gradients
        optimizer.zero_grad()

        # Forward pass
        relatedness_pred, entailment_pred = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)

        # Compute loss
        loss_1 = loss_fn_relatedness(relatedness_pred.squeeze(), relatedness_score)  # Regression loss
        loss_2 = loss_fn_entailment(entailment_pred, entailment_judgment)  # Classification loss
        loss = loss_1 + loss_2  # Total loss

        # alpha = loss_1 / (loss_1 + loss_2)  # 動態縮放因子
        # loss = alpha * loss_1 + (1 - alpha) * loss_2


        # print(f"relatedness_pred = {relatedness_pred.squeeze()}\nrelatedness_score = {relatedness_score}\n")
        # print(f"entailment_pred = {entailment_pred}\nentailment_judgment = {entailment_judgment}\n")


        # Backward pass
        loss.backward()

        # Optimizer step
        optimizer.step()

        # Update progress bar
        pbar.set_postfix(loss=loss.item())

    # Update lr
    scheduler.step(loss)
    current_lr = optimizer.param_groups[0]['lr']
    print(f"Epoch [{ep+1}/{epochs}] - Learning Rate after scheduler step: {current_lr}")


    # Validation loop
    pbar = tqdm(dl_validation)
    pbar.set_description(f"Validation epoch [{ep+1}/{epochs}]")
    model.eval()

    # Initialize evaluation metrics
    all_relatedness_preds = []
    all_relatedness_scores = []
    all_entailment_preds = []
    all_entailment_judgments = []

    # Validation loop
    for batch in pbar:
        input_ids = batch[0]['input_ids'].to(device)
        attention_mask = batch[0]['attention_mask'].to(device)
        token_type_ids = batch[0]['token_type_ids'].to(device)
        relatedness_score = batch[1].to(device)
        entailment_judgment = batch[2].to(device)

        # Forward pass without gradient computation
        with torch.no_grad():
            relatedness_pred, entailment_pred = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)

        # Store predictions and true labels for evaluation
        all_relatedness_preds.extend(relatedness_pred.squeeze().cpu().numpy())
        all_relatedness_scores.extend(relatedness_score.cpu().numpy())
        all_entailment_preds.extend(entailment_pred.argmax(dim=-1).cpu().numpy())  # Select the class with the highest probability
        all_entailment_judgments.extend(entailment_judgment.cpu().numpy())

    # GPT 生成 reset、update、compute
    spc.reset()
    acc.reset()
    f1.reset()

    # Update evaluation metrics using the defined functions
    spc.update(torch.tensor(all_relatedness_preds), torch.tensor(all_relatedness_scores))  # Spearman Correlation
    acc.update(torch.tensor(all_entailment_preds), torch.tensor(all_entailment_judgments))  # Accuracy
    f1.update(torch.tensor(all_entailment_preds), torch.tensor(all_entailment_judgments))  # F1 Score

    # Compute and print evaluation metrics
    spearman_corr = spc.compute()  # Spearman Correlation Coefficient
    accuracy = acc.compute()  # Accuracy
    f1_score = f1.compute()  # F1 Score


    print(f"Epoch [{ep+1}/{epochs}] -\nSpearman Corr: {spearman_corr:.4f}\nAccuracy: {accuracy:.4f}\nF1 Score: {f1_score:.4f}")

    # Save the model (if needed)
    # torch.save(model, f'{save_path}/ep{ep}.ckpt')

# for ep in range(epochs):
#     pbar = tqdm(dl_train)
#     pbar.set_description(f"Training epoch [{ep+1}/{epochs}]")
#     model.train()
#     # TODO4: Write the training loop
#     # Write your code here
#     # train your model
#     # clear gradient
#     # forward pass
#     # compute loss
#     # back-propagation
#     # model optimization

#     pbar = tqdm(dl_validation)
#     pbar.set_description(f"Validation epoch [{ep+1}/{epochs}]")
#     model.eval()
#     # TODO5: Write the evaluation loop
#     # Write your code here
#     # Evaluate your model
#     # Output all the evaluation scores (SpearmanCorrCoef, Accuracy, F1Score)
#     torch.save(model, f'./saved_models/ep{ep}.ckpt')

"""For test set predictions, you can write perform evaluation simlar to #TODO5."""

test_dataset = SemevalDataset(split="test")

dl_test = DataLoader(
    test_dataset,
    batch_size=validation_batch_size,
    shuffle=False,
    collate_fn=collate_fn
)

import torch
from torchmetrics import SpearmanCorrCoef, Accuracy, F1Score

# Evaluate the model on the test set
pbar = tqdm(dl_test)  # Assume you have a DataLoader named `dl_test`
model.eval()

# Initialize evaluation lists
all_relatedness_preds = []
all_relatedness_scores = []
all_entailment_preds = []
all_entailment_judgments = []

# Test loop
for batch in pbar:
    input_ids = batch[0]['input_ids'].to(device)
    attention_mask = batch[0]['attention_mask'].to(device)
    token_type_ids = batch[0]['token_type_ids'].to(device)
    relatedness_score = batch[1].to(device)
    entailment_judgment = batch[2].to(device)

    # No gradient computation, forward pass
    with torch.no_grad():
        relatedness_pred, entailment_pred = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)

    # Save the predictions and true labels for evaluation
    all_relatedness_preds.extend(relatedness_pred.squeeze().cpu().numpy())
    all_relatedness_scores.extend(relatedness_score.cpu().numpy())
    all_entailment_preds.extend(entailment_pred.argmax(dim=-1).cpu().numpy())  # Select the class with the highest probability
    all_entailment_judgments.extend(entailment_judgment.cpu().numpy())

# Convert lists to tensors for metric calculations
all_relatedness_preds = torch.tensor(all_relatedness_preds, dtype=torch.float32)
all_relatedness_scores = torch.tensor(all_relatedness_scores, dtype=torch.float32)
all_entailment_preds = torch.tensor(all_entailment_preds, dtype=torch.int64)
all_entailment_judgments = torch.tensor(all_entailment_judgments, dtype=torch.int64)

# GPT 生成 reset、update、compute
spc.reset()
acc.reset()
f1.reset()

# Update evaluation metrics using defined functions
spc.update(all_relatedness_preds, all_relatedness_scores)  # Spearman Correlation
acc.update(all_entailment_preds, all_entailment_judgments)  # Accuracy
f1.update(all_entailment_preds, all_entailment_judgments)  # F1 Score

# Compute and print evaluation metrics
spearman_corr = spc.compute()  # Spearman Correlation Coefficient
accuracy = acc.compute()  # Accuracy
f1_score = f1.compute()  # F1 Score

print(f"\nSpearman Corr: {spearman_corr:.4f}")
print(f"Accuracy: {accuracy:.4f}")
print(f"F1 Score: {f1_score:.4f}")

"""Compared with models trained separately on each of the sub-task,
does multi-output learning improve the performance?
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

# 1. 分類任務錯誤分析
def analyze_classification_errors(preds, labels, label_names):
    # 計算混淆矩陣
    cm = confusion_matrix(labels, preds)
    print("Classification Report:")
    print(classification_report(labels, preds, target_names=label_names))

    # 可視化混淆矩陣
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_names, yticklabels=label_names)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.show()

    # 找出錯誤樣本
    error_indices = np.where(preds != labels)[0]
    print(f"\nNumber of Misclassified Samples: {len(error_indices)}")

    return error_indices

# 2. 回歸任務錯誤分析
def analyze_regression_errors(preds, labels):
    # 計算誤差
    errors = np.abs(preds - labels)
    print(f"\nMean Absolute Error: {np.mean(errors):.4f}")
    print(f"Max Error: {np.max(errors):.4f}")

    # 可視化預測值與實際值的對比
    plt.figure(figsize=(8, 6))
    plt.scatter(labels, preds, alpha=0.5, color='blue')
    plt.plot([min(labels), max(labels)], [min(labels), max(labels)], color='red', linestyle='--')
    plt.xlabel('Actual Score')
    plt.ylabel('Predicted Score')
    plt.title('Regression Prediction vs Actual')
    plt.show()

    # 可視化誤差分布
    plt.figure(figsize=(8, 6))
    sns.histplot(errors, bins=30, kde=True, color='purple')
    plt.xlabel('Absolute Error')
    plt.title('Distribution of Absolute Errors')
    plt.show()

    # 找出誤差較大的樣本
    large_error_indices = np.argsort(errors)[-10:]  # 取誤差最大的10個樣本
    print(f"\nTop 10 Samples with Largest Errors: {large_error_indices}")

    return large_error_indices

# 3. 進行錯誤分析
label_names = ['NEUTRAL', 'ENTAILMENT', 'CONTRADICTION']

# 轉換為 numpy 數據格式
all_entailment_preds_np = np.array(all_entailment_preds)
all_entailment_labels_np = np.array(all_entailment_judgments)
all_relatedness_preds_np = np.array(all_relatedness_preds)
all_relatedness_scores_np = np.array(all_relatedness_scores)

# 分析分類錯誤
classification_errors = analyze_classification_errors(all_entailment_preds_np, all_entailment_labels_np, label_names)

# 分析回歸錯誤
regression_errors = analyze_regression_errors(all_relatedness_preds_np, all_relatedness_scores_np)

import torch
import torch.nn as nn
import torch.optim as optim
from transformers import BertModel, BertTokenizer

# 定義回歸模型 (model1)
class RegressionModel(nn.Module):
    def __init__(self, pretrained_model_name="bert-base-uncased"):
        super(RegressionModel, self).__init__()
        # 使用 BERT 作為嵌入層
        self.bert = BertModel.from_pretrained(pretrained_model_name)
        self.regressor = nn.Linear(self.bert.config.hidden_size, 1)  # 回歸任務，輸出一個數字

    def forward(self, input_ids, attention_mask, token_type_ids):
        # 通過 BERT 獲取隱藏層輸出
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
        hidden_state = outputs.last_hidden_state  # [batch_size, sequence_length, hidden_size]

        # 取[CLS] token的隱藏狀態作為回歸的特徵
        cls_token_state = hidden_state[:, 0, :]  # [batch_size, hidden_size]

        # 回歸層
        regression_output = self.regressor(cls_token_state)
        return regression_output


# 定義分類模型 (model2)
class ClassificationModel(nn.Module):
    def __init__(self, pretrained_model_name="bert-base-uncased"):
        super(ClassificationModel, self).__init__()
        # 使用 BERT 作為嵌入層
        self.bert = BertModel.from_pretrained(pretrained_model_name)
        self.classifier = nn.Linear(self.bert.config.hidden_size, 3)  # 分類任務，3類（假設是三分類）

    def forward(self, input_ids, attention_mask, token_type_ids):
        # 通過 BERT 獲取隱藏層輸出
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
        hidden_state = outputs.last_hidden_state  # [batch_size, sequence_length, hidden_size]

        # 取[CLS] token的隱藏狀態作為分類的特徵
        cls_token_state = hidden_state[:, 0, :]  # [batch_size, hidden_size]

        # 分類層
        classification_output = self.classifier(cls_token_state)
        return classification_output


# 初始化兩個模型
model1 = RegressionModel(pretrained_model_name="bert-base-uncased").to(device)  # 回歸模型
model2 = ClassificationModel(pretrained_model_name="bert-base-uncased").to(device)  # 分類模型

# 初始化優化器
optimizer1 = optim.Adam(model1.parameters(), lr=lr)
optimizer2 = optim.Adam(model2.parameters(), lr=lr)

# 定義損失函數
loss_fn_relatedness = nn.MSELoss()  # 回歸損失
loss_fn_entailment = nn.CrossEntropyLoss()  # 分類損失

"""model1"""

from sklearn.metrics import accuracy_score, f1_score
from scipy.stats import spearmanr
from torchmetrics import SpearmanCorrCoef, Accuracy, F1Score
import torch
from tqdm import tqdm

# 初始化 SpearmanCorrCoef
spc = SpearmanCorrCoef().to(device)

save_path = "./drive/MyDrive/saved_models"

for ep in range(epochs):
    pbar = tqdm(dl_train)
    pbar.set_description(f"Training epoch [{ep+1}/{epochs}]")
    model1.train()

    # Training loop (回歸任務)
    for batch in pbar:
        # Extract data
        input_ids = batch[0]['input_ids'].to(device)
        attention_mask = batch[0]['attention_mask'].to(device)
        token_type_ids = batch[0]['token_type_ids'].to(device)
        relatedness_score = batch[1].to(device)

        # Zero the gradients
        optimizer1.zero_grad()

        # Forward pass (回歸任務)
        relatedness_pred = model1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)

        # Compute loss for regression task (回歸損失)
        loss_1 = loss_fn_relatedness(relatedness_pred.squeeze(), relatedness_score)  # Regression loss

        # Backward pass
        loss_1.backward()

        # Optimizer step
        optimizer1.step()

        # Update progress bar
        pbar.set_postfix(loss=loss_1.item())

    # Validation loop
    pbar = tqdm(dl_validation)
    pbar.set_description(f"Validation epoch [{ep+1}/{epochs}]")
    model1.eval()

    # Initialize evaluation metrics
    all_relatedness_preds = []
    all_relatedness_scores = []

    # Validation loop
    for batch in pbar:
        input_ids = batch[0]['input_ids'].to(device)
        attention_mask = batch[0]['attention_mask'].to(device)
        token_type_ids = batch[0]['token_type_ids'].to(device)
        relatedness_score = batch[1].to(device)

        # Forward pass without gradient computation
        with torch.no_grad():
            relatedness_pred= model1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)

        # Store predictions and true labels for evaluation
        all_relatedness_preds.extend(relatedness_pred.squeeze().cpu().numpy())
        all_relatedness_scores.extend(relatedness_score.cpu().numpy())

    # GPT 生成 reset、update、compute

    spc.reset()
    # Update evaluation metrics using the defined functions
    spc.update(torch.tensor(all_relatedness_preds), torch.tensor(all_relatedness_scores))  # Spearman Correlation

    # Compute and print evaluation metrics
    spearman_corr = spc.compute()  # Spearman Correlation Coefficient
    print(f"Epoch [{ep+1}/{epochs}] -\nSpearman Corr: {spearman_corr:.4f}")

    # Save the model (if needed)
    # torch.save(model1, f'{save_path}/ep{ep}.ckpt')

from sklearn.metrics import accuracy_score, f1_score
from scipy.stats import spearmanr
from torchmetrics import SpearmanCorrCoef, Accuracy, F1Score
import torch
from tqdm import tqdm


save_path = "./drive/MyDrive/saved_models"

for ep in range(epochs):
    pbar = tqdm(dl_train)
    pbar.set_description(f"Training classification epoch [{ep+1}/{epochs}]")
    model2.train()

    # 分類任務訓練循環
    for batch in pbar:
        # Extract data
        input_ids = batch[0]['input_ids'].to(device)
        attention_mask = batch[0]['attention_mask'].to(device)
        token_type_ids = batch[0]['token_type_ids'].to(device)
        entailment_label = batch[2].to(device)

        # Zero the gradients
        optimizer2.zero_grad()

        # Forward pass (分類任務)
        entailment_pred = model2(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)

        # Compute loss for classification task (分類損失)
        loss_2 = loss_fn_entailment(entailment_pred, entailment_label)  # Classification loss

        # Backward pass
        loss_2.backward()

        # Optimizer step
        optimizer2.step()

        # Update progress bar
        pbar.set_postfix(loss=loss_2.item())

    # --------- 驗證分類模型 (model2) ---------
    pbar = tqdm(dl_validation)
    pbar.set_description(f"Validation classification epoch [{ep+1}/{epochs}]")
    model2.eval()

    # Initialize evaluation metrics for classification
    all_entailment_preds = []
    all_entailment_labels = []

    # Validation loop for classification
    for batch in pbar:
        input_ids = batch[0]['input_ids'].to(device)
        attention_mask = batch[0]['attention_mask'].to(device)
        token_type_ids = batch[0]['token_type_ids'].to(device)
        entailment_label = batch[2].to(device)

        # Forward pass without gradient computation
        with torch.no_grad():
            entailment_pred = model2(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)

        # Store predictions and true labels for evaluation
        all_entailment_preds.extend(entailment_pred.argmax(dim=-1).cpu().numpy())
        all_entailment_labels.extend(entailment_label.cpu().numpy())

    # Convert predictions and labels to tensor
    all_entailment_preds = torch.tensor(all_entailment_preds, dtype=torch.int64)
    all_entailment_labels = torch.tensor(all_entailment_labels, dtype=torch.int64)

    # GPT 生成 reset、update、compute
    acc.reset()
    f1.reset()

    # Update evaluation metrics using defined functions
    acc.update(all_entailment_preds, all_entailment_labels)  # Accuracy
    f1.update(all_entailment_preds, all_entailment_labels)  # F1 Score

    # Compute accuracy and F1 score
    accuracy = acc.compute()  # Accuracy
    f1_score_val = f1.compute()  # F1 Score

    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1 Score: {f1_score_val:.4f}")

    # Save the model (if needed)
    # torch.save(model2, f'{save_path}/ep{ep}.ckpt')

"""test data"""

import torch
from torchmetrics import SpearmanCorrCoef, Accuracy, F1Score

# Initialize metrics (make sure the metric is properly initialized)
spc = SpearmanCorrCoef()
acc = Accuracy(task="multiclass", num_classes=3)
f1 = F1Score(task="multiclass", num_classes=3, average='macro')

# Evaluate the model on the test set
pbar = tqdm(dl_test)  # Assume you have a DataLoader named `dl_test`
model.eval()

# Initialize evaluation lists
all_relatedness_preds = []
all_relatedness_scores = []
all_entailment_preds = []
all_entailment_judgments = []

# Test loop
for batch in pbar:
    input_ids = batch[0]['input_ids'].to(device)
    attention_mask = batch[0]['attention_mask'].to(device)
    token_type_ids = batch[0]['token_type_ids'].to(device)
    relatedness_score = batch[1].to(device)
    entailment_judgment = batch[2].to(device)

    # No gradient computation, forward pass
    with torch.no_grad():
        relatedness_pred = model1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
        entailment_pred = model2(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)

    # Save the predictions and true labels for evaluation
    all_relatedness_preds.extend(relatedness_pred.squeeze().cpu().numpy())
    all_relatedness_scores.extend(relatedness_score.cpu().numpy())
    all_entailment_preds.extend(entailment_pred.argmax(dim=-1).cpu().numpy())  # Select the class with the highest probability
    all_entailment_judgments.extend(entailment_judgment.cpu().numpy())

# Convert lists to tensors for metric calculations
all_relatedness_preds = torch.tensor(all_relatedness_preds, dtype=torch.float32)
all_relatedness_scores = torch.tensor(all_relatedness_scores, dtype=torch.float32)
all_entailment_preds = torch.tensor(all_entailment_preds, dtype=torch.int64)
all_entailment_judgments = torch.tensor(all_entailment_judgments, dtype=torch.int64)\

# GPT 生成 reset、update、compute
spc.reset()
acc.reset()
f1.reset()

# Update evaluation metrics using defined functions
spc.update(all_relatedness_preds, all_relatedness_scores)  # Spearman Correlation
acc.update(all_entailment_preds, all_entailment_judgments)  # Accuracy
f1.update(all_entailment_preds, all_entailment_judgments)  # F1 Score

# Compute and print evaluation metrics
spearman_corr = spc.compute()  # Spearman Correlation Coefficient
accuracy = acc.compute()  # Accuracy
f1_score = f1.compute()  # F1 Score

print(f"\nSpearman Corr: {spearman_corr:.4f}")
print(f"Accuracy: {accuracy:.4f}")
print(f"F1 Score: {f1_score:.4f}")